{"cells":[{"cell_type":"markdown","id":"0520b491","metadata":{"id":"0520b491"},"source":["# $\\color{ForestGreen}{\\text{Regression through PyTorch}}$"]},{"cell_type":"code","execution_count":null,"id":"2e03cc50","metadata":{"id":"2e03cc50"},"outputs":[],"source":["# Importing basic libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","from tqdm import tqdm\n","import time\n","\n","# Importing some basic modules in torch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import BatchSampler, Dataset, DataLoader"]},{"cell_type":"markdown","source":["In deep learning, GPUs are commonly used to fortify the computational power and reduce the computational time. The following function moves the data from CPU to a GPU device if one is provided. But today, let's just use CPU."],"metadata":{"id":"X_nj6o-aa2hH"},"id":"X_nj6o-aa2hH"},{"cell_type":"code","execution_count":null,"id":"143f41ec","metadata":{"id":"143f41ec"},"outputs":[],"source":["def moveTo(obj, device):\n","    if isinstance(obj, list):\n","        return [moveTo(x, device) for x in obj]\n","    elif isinstance(obj, tuple):\n","        return tuple(moveTo(list(obj), device))\n","    elif isinstance(obj, set):\n","        return set(moveTo(list(obj), device))\n","    elif isinstance(obj, dict):\n","        to_ret = dict()\n","        for key, value in obj.items():\n","            to_ret[moveTo(key, device)] = moveTo(value, device)\n","        return to_ret\n","    elif hasattr(obj, \"to\"):\n","        return obj.to(device)\n","    else:\n","        return obj"]},{"cell_type":"markdown","source":["The following function function calculates the gradient of a given loss function and updates the parameters of the given model for one epoch"],"metadata":{"id":"x3UTTvFha8Cw"},"id":"x3UTTvFha8Cw"},{"cell_type":"code","execution_count":null,"id":"66c60e81","metadata":{"id":"66c60e81"},"outputs":[],"source":["def run_epoch(model, optimizer, data_loader, loss_func, device, results, score_funcs, prefix=\"\", desc=None):\n","    running_loss = []\n","    y_true = []\n","    y_pred = []\n","    start = time.time()\n","    for inputs, labels in tqdm(data_loader, desc=desc, leave=False):\n","        #Move the batch to the device we are using. \n","        inputs = moveTo(inputs, device)\n","        labels = moveTo(labels, device)\n","        \n","        y_hat = model(inputs) \n","\n","        # Compute loss.\n","        loss = loss_func(y_hat, labels)\n","         \n","        if model.training:\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        #Now we are just grabbing some information we would like to have\n","        running_loss.append(loss.item())\n","\n","        if len(score_funcs) > 0 and isinstance(labels, torch.Tensor):\n","            #moving labels & predictions back to CPU for computing / storing predictions\n","            labels = labels.detach().cpu().numpy()\n","            y_hat = y_hat.detach().cpu().numpy()\n","            #add to predictions so far\n","            y_true.extend(labels.tolist())\n","            y_pred.extend(y_hat.tolist())\n","    #end training epoch\n","    end = time.time()\n","    \n","    y_pred = np.asarray(y_pred)\n","    \n","    results[prefix + \" loss\"].append( np.mean(running_loss) )\n","    for name, score_func in score_funcs.items():\n","        try:\n","            results[prefix + \" \" + name].append( score_func(y_true, y_pred) )  # For regression \n","        except:\n","            results[prefix + \" \" + name].append(score_func(y_true, np.argmax(y_pred, axis=1))) # For classification\n","    return end-start #time spent on epoch"]},{"cell_type":"markdown","source":["The following function trains a given model and calculate the specified relevant performance metrics for a number of (specified) epochs. Furthermore, it collects the calculated metrics for all the epochs in a dataframe."],"metadata":{"id":"ro7dhdnJdjUW"},"id":"ro7dhdnJdjUW"},{"cell_type":"code","execution_count":null,"id":"70b82bb0","metadata":{"id":"70b82bb0"},"outputs":[],"source":["def train_simple_network_alt(model, optimizer, loss_func, eta, train_loader, test_loader=None, score_funcs=None, \n","                         epochs=50, device=\"cpu\"):\n","    to_track = [\"epoch\", \"total time\", \"train loss\"]\n","    if test_loader is not None:\n","        to_track.append(\"test loss\")\n","    if score_funcs is not None:\n","        for eval_score in score_funcs:\n","            to_track.append(\"train \" + eval_score )\n","            if test_loader is not None:\n","                to_track.append(\"test \" + eval_score )\n","        \n","    total_train_time = 0              #How long have we spent in the training loop? \n","    results = {}\n","    #Initialize every item with an empty list\n","    for item in to_track:\n","        results[item] = []\n","        \n","    #Place the model on the correct compute resource (CPU or GPU)\n","    model.to(device)\n","    for epoch in tqdm(range(1, epochs+1), desc=\"Epoch\"):\n","        model = model.train()#Put our model in training mode\n","        \n","        total_train_time += run_epoch(model, optimizer, train_loader, loss_func, device, results, score_funcs, \n","                                      prefix=\"train\", desc=\"Training\")  \n","        results[\"total time\"].append( total_train_time )\n","        results[\"epoch\"].append( epoch )\n","        \n","        if test_loader is not None:\n","            model = model.eval()\n","            with torch.no_grad():\n","                run_epoch(model, optimizer, test_loader, loss_func, device, results, score_funcs, prefix=\"test\", \n","                          desc=\"Testing\")\n","                \n","    return pd.DataFrame.from_dict(results)"]},{"cell_type":"markdown","id":"ca056791","metadata":{"id":"ca056791"},"source":["## $\\color{ForestGreen}{\\text{A Toy Dataset for Regression}}$"]},{"cell_type":"code","execution_count":null,"id":"df93fdc7","metadata":{"id":"df93fdc7"},"outputs":[],"source":["X = np.linspace(0, 30, num=300)\n","#create an output\n","y = -X + np.cos(X)*3 + np.random.normal(0, 1, size=X.shape) # Adding some noise coming from a normal distribution\n","sns.scatterplot(x=X, y=y)\n","plt.title('Graph of X vs y')\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.gcf().set_size_inches(7, 5)\n","plt.show()"]},{"cell_type":"markdown","source":["The following class takes the feature and the target as\n","numpy arrays and converts them into a torch.tensor."],"metadata":{"id":"D2yLmvWud3Wo"},"id":"D2yLmvWud3Wo"},{"cell_type":"code","execution_count":null,"id":"96e77af5","metadata":{"id":"96e77af5"},"outputs":[],"source":["class RegressionDataset(Dataset):        \n","    def __init__(self, X, y):\n","        self.X = X.reshape(-1,1)\n","        self.y = y.reshape(-1,1)\n","    \n","    def __getitem__(self, index):\n","        return torch.tensor(self.X[index,:], dtype=torch.float32), torch.tensor(self.y[index], dtype=torch.float32)\n","\n","    def __len__(self):\n","        return self.X.shape[0]"]},{"cell_type":"code","execution_count":null,"id":"2e1da2e0","metadata":{"id":"2e1da2e0"},"outputs":[],"source":["# We use 'torch.utils.data.random_split' to split the data in pytorch into train and test subsets\n","data = RegressionDataset(X, y)\n","\n","train_data, test_data = torch.utils.data.random_split(data, (len(data)-50, 50)) "]},{"cell_type":"code","execution_count":null,"id":"331e4678","metadata":{"id":"331e4678"},"outputs":[],"source":["# Loading the data through 'DataLoader':\n","train_loader = DataLoader(train_data , shuffle=True)\n","test_loader = DataLoader(test_data, shuffle=False)"]},{"cell_type":"code","execution_count":null,"id":"2bddcc2b","metadata":{"id":"2bddcc2b"},"outputs":[],"source":["# Building the model\n","n_features = 1   # Number of features\n","n_targets = 1    # Number of targets\n","\n","eta=0.001        # Learning rate\n","\n","#del model_1\n","model_1 = nn.Sequential(\n","    nn.Linear(n_features,  n_targets),  # One layer directly from features to the targets (no hidden layers)\n",")\n","\n","optimizer = torch.optim.Adam(model_1.parameters(), lr=eta)  # Adam is a very common optimizer used in DL\n","\n","loss_func = nn.MSELoss()   # We choose the loss function to be MSE "]},{"cell_type":"code","execution_count":null,"id":"a43ad945","metadata":{"id":"a43ad945"},"outputs":[],"source":["# Training the model and collect the relevant performance metrics \n","fc_1_results = train_simple_network_alt(model_1, optimizer, loss_func, eta, train_loader, test_loader, \n","                         score_funcs={'accuracy':r2_score}, epochs=25)"]},{"cell_type":"code","execution_count":null,"id":"38aa639c","metadata":{"id":"38aa639c"},"outputs":[],"source":["# Displaying the calculated metrics\n","fc_1_results"]},{"cell_type":"code","execution_count":null,"id":"b757e788","metadata":{"id":"b757e788"},"outputs":[],"source":["# Plotting the loss function of the fully connected model per epoch:\n","sns.lineplot(x='epoch', y='train loss', data=fc_1_results[1:], label='train loss')\n","sns.lineplot(x='epoch', y='test loss', data=fc_1_results[1:], label='test loss')\n","plt.title('MSE Loss Function of Model 1')\n","plt.gcf().set_size_inches(7, 5)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"3ba681d4","metadata":{"id":"3ba681d4"},"outputs":[],"source":["# Plotting the R^2 score of the fully connected model per epoch:\n","sns.lineplot(x='epoch', y='train accuracy', data=fc_1_results[6:], label=r'train $R^2$')\n","sns.lineplot(x='epoch', y='test accuracy', data=fc_1_results[6:], label=r'test $R^2$')\n","plt.title(r'$R^2$ Score of Model 1')\n","plt.gcf().set_size_inches(7, 5)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"9a528478","metadata":{"id":"9a528478"},"outputs":[],"source":["# Visualizing the result of Model 1 (linear regression through a neural network)\n","with torch.no_grad():\n","    Y_pred = model_1(torch.tensor(X.reshape(-1,1), dtype=torch.float32)).cpu().numpy() #Shape of (N, 1)\n","\n","# Note: numpy.ravel(a, order='C') returns a contiguous flattened array.    \n","\n","sns.scatterplot(x=X, y=y, color='blue', label='Data') #The data\n","sns.lineplot(x=X, y=Y_pred.ravel(), color='red', label='Model') #What our model learned\n","plt.title('Single Linear Layer Prediction')\n","plt.gcf().set_size_inches(7, 5)\n","plt.show()"]},{"cell_type":"markdown","source":["Now let's use three linear layers!"],"metadata":{"id":"r5f3rBt_c7sB"},"id":"r5f3rBt_c7sB"},{"cell_type":"code","execution_count":null,"id":"2a607035","metadata":{"id":"2a607035"},"outputs":[],"source":["n_features = 1  # Number of features\n","n_targets = 1   # Number of targets\n","\n","n_neurons = 32  # Number of neurons used in hidden layers\n","\n","eta=0.001       # Learning rate\n","\n","#del model_2\n","model_2 = nn.Sequential(\n","    nn.Linear(n_features,  n_neurons),  # From input layer to hidden layer 1\n","    nn.Linear(n_neurons,  n_neurons),   # From hidde layer 1 to hidden layer 2\n","    nn.Linear(n_neurons,  n_neurons),   # From hidde layer 2 to hidden layer 3\n","    nn.Linear(n_neurons,  n_targets),   # From hidde layer 3 to output layer \n",")\n","\n","optimizer = torch.optim.Adam(model_2.parameters(), lr=eta)   # Adam optimizer \n","\n","loss_func = nn.MSELoss()  # We choose the loss function to be MSE"]},{"cell_type":"code","execution_count":null,"id":"6172d4e1","metadata":{"id":"6172d4e1"},"outputs":[],"source":["# Training the model and collect the relevant performance metrics \n","\n","fc_2_results = train_simple_network_alt(model_2, optimizer, loss_func, eta, train_loader, test_loader, \n","                         score_funcs={'accuracy':r2_score}, epochs=25)"]},{"cell_type":"code","execution_count":null,"id":"66f40d02","metadata":{"id":"66f40d02"},"outputs":[],"source":["# Displaying the calculated metrics\n","\n","fc_2_results"]},{"cell_type":"code","execution_count":null,"id":"17ad47a0","metadata":{"id":"17ad47a0"},"outputs":[],"source":["# Visualizing the result of Model 2 (linear regression through a neural network)\n","\n","with torch.no_grad():\n","    Y_pred = model_2(torch.tensor(X.reshape(-1,1), dtype=torch.float32)).cpu().numpy() #Shape of (N, 1)\n","    \n","sns.scatterplot(x=X, y=y, color='blue', label='Data') #The data\n","sns.lineplot(x=X, y=Y_pred.ravel(), color='red', label='Model') #What our model learned\n","plt.title('Multiple Linear Layers Prediction')\n","plt.gcf().set_size_inches(7, 5)\n","plt.show()"]},{"cell_type":"markdown","source":["Now, let's add nonlinearity to our network!"],"metadata":{"id":"M6ZEE3ILdRo0"},"id":"M6ZEE3ILdRo0"},{"cell_type":"code","execution_count":null,"id":"67a4556f","metadata":{"id":"67a4556f"},"outputs":[],"source":["n_features = 1  # Number of features\n","n_targets = 1   # Number of targets\n","\n","n_neurons = 64  # Number of neurons used in hidden layers\n","\n","eta=0.001       # Learning rate\n","\n","#del model_3\n","model_3 = nn.Sequential(\n","    nn.Linear(n_features,  n_neurons),   # From input layer to hidden layer 1\n","    nn.Tanh(),                           # Applying tanh activation function\n","    nn.Linear(n_neurons,  n_neurons),    # From hidde layer 1 to hidden layer 2\n","    nn.Tanh(),                           # Applying tanh activation function \n","    nn.Linear(n_neurons,  n_neurons),    # From hidde layer 2 to hidden layer 3\n","    nn.Tanh(),                           # Applying tanh activation function\n","    nn.Linear(n_neurons,  n_targets),    # From hidde layer 3 to output layer \n",")\n","\n","optimizer = torch.optim.Adam(model_3.parameters(), lr=eta)    # Adam optimizer \n","\n","loss_func = nn.MSELoss()                 # We choose the loss function to be MSE"]},{"cell_type":"code","execution_count":null,"id":"d9d77940","metadata":{"id":"d9d77940"},"outputs":[],"source":["# Training the model and collect the relevant performance metrics \n","\n","fc_3_results = train_simple_network_alt(model_3, optimizer, loss_func, eta, train_loader, test_loader, \n","                         score_funcs={'accuracy':r2_score}, epochs=35)"]},{"cell_type":"code","execution_count":null,"id":"138227ac","metadata":{"id":"138227ac"},"outputs":[],"source":["# Visualizing the result of Model 3 (neural network in presence of activation functions)\n","\n","with torch.no_grad():\n","    Y_pred = model_3(torch.tensor(X.reshape(-1,1), dtype=torch.float32)).cpu().numpy() #Shape of (N, 1)\n","    \n","sns.scatterplot(x=X, y=y, color='blue', label='Data') #The data\n","sns.lineplot(x=X, y=Y_pred.ravel(), color='red', label='Model') #What our model learned\n","plt.title('Predictions in the Presence of Activations')\n","plt.gcf().set_size_inches(7, 5)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"8f230703","metadata":{"id":"8f230703"},"outputs":[],"source":["# Displaying the calculated metrics\n","\n","fc_3_results[-10:]"]},{"cell_type":"code","execution_count":null,"id":"782487ec","metadata":{"id":"782487ec"},"outputs":[],"source":["# Plotting the accuracy of model 3:\n","\n","sns.lineplot(x='epoch', y='train accuracy', data=fc_3_results[1:], label=r'train $R^2$')\n","sns.lineplot(x='epoch', y='test accuracy', data=fc_3_results[1:], label=r'test $R^2$')\n","plt.title('Accuracy of Model 3')\n","plt.gcf().set_size_inches(7, 5)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"53c334ba","metadata":{"id":"53c334ba"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"colab":{"provenance":[]},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}